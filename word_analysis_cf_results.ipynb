{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/fatma/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (string):\n",
    "    regex_str = [\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "    r'(?!)'      \n",
    "    ]\n",
    "    \n",
    "    tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "    \n",
    "    #string = string.split()\n",
    "    string = tokens_re.findall(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_words(string):\n",
    "    clean_string = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    string = string.replace(\"#\", \"\")\n",
    "    string = clean_text(string)\n",
    "    unwanted_words = ['rt', 'usrId', 'DD', 'httpAddress',\"'s\",\"'d\", \"turkey\", \"turkish\",\"occupygezi\",\\\n",
    "                      \"gezi\", \"taksim\", \"jun\", \"square\",\"istanbul\", \"park\", \"direngeziparki\",\"D\",\"pm\", \"am\"]\n",
    "    for i in string:\n",
    "        if i not in unwanted_words:\n",
    "            if not i.isdigit():\n",
    "                if i not in stop:\n",
    "                    clean_string.append(i)\n",
    "\n",
    "    return clean_string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_data_df = pd.read_csv(\"Data/CF_aggregated_results_09_05_2018.csv\")\n",
    "CF_data_df.violence.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CF_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = CF_data_df[CF_data_df[\"violence\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = CF_data_df[CF_data_df[\"violence\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = [remove_unwanted_words(i) for i in pos_df.proccd_text]\n",
    "neg_list = [remove_unwanted_words(i) for i in neg_df.proccd_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_list = [item for items in pos_list for item in items] #flatten the list of lists into one list\n",
    "neg_words_list = [item for items in neg_list for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies(word_list):\n",
    "    freq={}\n",
    "\n",
    "    for word in word_list:\n",
    "        count=freq.get(word,0)\n",
    "        freq[word]=count + 1\n",
    "\n",
    "    frequency_list  = freq.keys()    \n",
    "\n",
    "    results = []\n",
    "    for word in frequency_list:\n",
    "        tuple = (word, freq[word])\n",
    "        results.append(tuple)\n",
    "\n",
    "    byFreq=sorted(results, key=lambda word: word[1], reverse=True)\n",
    "\n",
    "\n",
    "    words_names=[]\n",
    "    words_count=[]\n",
    "    for (word, freq) in byFreq[:10]:\n",
    "        print (word, freq)\n",
    "        words_names.append(word)\n",
    "        words_count.append(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "police 101\n",
      "gas 44\n",
      "tear 35\n",
      "protesters 30\n",
      "people 22\n",
      "violence 15\n",
      "protests 13\n",
      "water 11\n",
      "photo 11\n",
      "attack 10\n"
     ]
    }
   ],
   "source": [
    "pos = word_frequencies(pos_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erdogan 122\n",
      "protests 112\n",
      "police 99\n",
      "via 69\n",
      "protesters 67\n",
      "people 67\n",
      "world 44\n",
      "media 40\n",
      "protest 38\n",
      "news 34\n"
     ]
    }
   ],
   "source": [
    "neg = word_frequencies(neg_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-159-5c856bd00cc7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-159-5c856bd00cc7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def plot_freq()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_freq()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Plot histogram using matplotlib bar()\n",
    "plt.xlabel('Top 10 Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Plotting Word Frequency')\n",
    "indexes = np.arange(len(words_names) )\n",
    "width = .4\n",
    "plt.bar(indexes, words_count, width)\n",
    "plt.xticks(indexes + width * .4, words_names)\n",
    "#plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
