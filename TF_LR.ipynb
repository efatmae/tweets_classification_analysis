{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fatma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \", string)\n",
    "    string = re.sub(r\"n\\'t\", \" \", string)\n",
    "    string = re.sub(r\"n\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'re\", \" \", string)\n",
    "    string = re.sub(r\"\\'d\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" \", string)\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\?\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\//?\", \" \", string)\n",
    "    string = re.sub(r\"\\d+\", \" \", string)\n",
    "    string = re.sub(r\"\\$\", \" \", string)\n",
    "    string = re.sub(r\"\\#\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(str):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    lst = str.split()\n",
    "    lst = [i for i in lst if i not in stop]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def stem (str):\n",
    "    lst = str.split()\n",
    "    lst = [st.stem(x) for x in lst]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def remove_unwanted_words(str):\n",
    "    unwanted_words = [\"httpaddress\", \"usrid\", \"D\", \"dd\", \"rt\", \"amp\", \"am\", \"pm\", '``',\n",
    "                      \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\", \"\\?\",\"http\",\"httpaddresshttpaddresst\", \"cohttpaddressek\",\n",
    "                      \"taksim\",\"gezi\", \"park\", \"direngeziparki\", \"occupygezi\", \"istanbul\", \"turkish\",\"turkey\",\n",
    "                      \"protest\",\"direngezipark\",\"direnankara\",\"geziparki\", \"protesters\", \"protests\", \"sat\", \"sun\", \"mon,\",\n",
    "                     \"tue\", \"wed\", \"thu\", \"fri\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    lst = str.split(\" \")\n",
    "    lst = [i for i in lst if i not in unwanted_words]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def toLower(str):\n",
    "    lst = str.split()\n",
    "    lst = [i.lower() for i in lst]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def word_len (str):\n",
    "    lst = str.split()\n",
    "    lst = [i for i in lst if len(i)>1 and len(i) <7]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def sent_len (str):\n",
    "    lst = str.split()\n",
    "    if len(lst)>=3:\n",
    "        return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels_shuffled(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity african_data from files, splits the african_data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load african_data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    positive_examples = [remove_stop(item) for item in positive_examples]\n",
    "    #print(positive_examples[0])\n",
    "    positive_examples = [toLower(item) for item in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    positive_examples = [clean_str(sent) for sent in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    #positive_examples = [stem(item) for item in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    positive_examples = [remove_unwanted_words(item) for item in positive_examples]\n",
    "    positive_examples = [word_len(item) for item in positive_examples]\n",
    "    positive_examples = [sent_len(item) for item in positive_examples]\n",
    "    positive_examples = list(filter(None, positive_examples))\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    negative_examples = [toLower(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [remove_stop(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [clean_str(sent) for sent in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    #ngative_examples = [stem(sent) for sent in negative_examples]\n",
    "    #pint(negative_examples[0])\n",
    "    negative_examples = [remove_unwanted_words(item) for item in negative_examples]\n",
    "    negative_examples = [word_len(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [sent_len(item) for item in negative_examples]\n",
    "\n",
    "    negative_examples = list(filter(None, negative_examples))\n",
    "\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    # x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Choose max text word length at 25\n",
    "sentence_size = 10\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text, tweets_y = load_data_and_labels_shuffled('Data/CF_Fatma_label_pos.txt', 'Data/CF_Fatma_label_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_tweets_DS_X_train, CF_tweets_DS_X_test, CF_tweets_DS_y_train, CF_tweets_DS_y_test = train_test_split(np.array(tweets_text),np.array(tweets_y),\n",
    "test_size= 0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source training dataset vocabulary size 152\n",
      "source test dataset vocabulary size 129\n"
     ]
    }
   ],
   "source": [
    "#source train dataset Build vocabulary\n",
    "vocab_processor_CF_training_text = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "CF_tweets_DS_x_train = np.array(list(vocab_processor_CF_training_text.fit_transform(CF_tweets_DS_X_train)))\n",
    "print(\"source training dataset vocabulary size\", len(vocab_processor_CF_training_text.vocabulary_))\n",
    "training_vocab_size = len(vocab_processor_CF_training_text.vocabulary_)\n",
    "\n",
    "#source test dataset Build vocabulary\n",
    "vocab_processor_CF_test_text = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "CF_tweets_DS_x_test = np.array(list(vocab_processor_CF_test_text.fit_transform(CF_tweets_DS_X_test)))\n",
    "test_vocab_size = len(vocab_processor_CF_test_text.vocabulary_)\n",
    "print(\"source test dataset vocabulary size\", test_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up data set into train/test\n",
    "#train_indices = np.random.choice(len(tweets_text), round(len(tweets_text) * 0.7), replace=False)\n",
    "#test_indices = np.array(list(set(range(len(tweets_text))) - set(train_indices)))\n",
    "#texts_train = [x for ix, x in enumerate(tweets_text) if ix in train_indices]\n",
    "#texts_test = [x for ix, x in enumerate(tweets_text) if ix in test_indices]\n",
    "#target_train = [x for ix, x in enumerate(tweets_y) if ix in train_indices]\n",
    "#target_test = [x for ix, x in enumerate(tweets_y) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 498\n",
      "positive samples 97\n",
      "negative samples 401\n",
      "test size 499\n",
      "positive samples 111\n",
      "negative samples 388\n"
     ]
    }
   ],
   "source": [
    "print(\"train size\", len(CF_tweets_DS_X_train))\n",
    "print(\"positive samples\", list(CF_tweets_DS_y_train).count([1]))\n",
    "print(\"negative samples\", list(CF_tweets_DS_y_train).count([0]))\n",
    "\n",
    "print(\"test size\", len(CF_tweets_DS_X_test))\n",
    "print(\"positive samples\",  list(CF_tweets_DS_y_test).count([1]))\n",
    "print(\"negative samples\", list(CF_tweets_DS_y_test).count([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in vocab_processor_CF_training_text.transform(CF_tweets_DS_X_train)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Index Matrix for one-hot-encoding\n",
    "identity_mat = tf.diag(tf.ones(shape=[training_vocab_size,len(CF_tweets_DS_X_train)]))\n",
    "\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[training_vocab_size, 1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[training_vocab_size, len(CF_tweets_DS_X_train)], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, len(CF_tweets_DS_y_train)], dtype=tf.float32)\n",
    "\n",
    "\n",
    "x_test_data = tf.placeholder(shape=[training_vocab_size, len(CF_tweets_DS_X_test)], dtype=tf.int32)\n",
    "y_test_target = tf.placeholder(shape=[1, len(CF_tweets_DS_y_test)], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-Vocab Embedding\n",
    "x_embed = tf.contrib.layers.bow_encoder(x_data, vocab_size=training_vocab_size, embed_dim=128)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 128 and 1 for 'MatMul_3' (op: 'MatMul') with input shapes: [1,128], [1,152].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 128 and 1 for 'MatMul_3' (op: 'MatMul') with input shapes: [1,128], [1,152].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-941fe0e62c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Declare model operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_col_sums_2D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_col_sums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_col_sums_2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Declare loss function (Cross Entropy loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1844\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \"\"\"\n\u001b[1;32m   1288\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1289\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1290\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 128 and 1 for 'MatMul_3' (op: 'MatMul') with input shapes: [1,128], [1,152]."
     ]
    }
   ],
   "source": [
    "# Declare model operations\n",
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n",
    "\n",
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Prediction operation\n",
    "prediction = tf.sigmoid(model_output)\n",
    "_, label_auc = tf.metrics.auc(y_target, prediction)\n",
    "\n",
    "# Declare optimizer\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)\n",
    "\n",
    "# Intitialize Variables\n",
    "init =  tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Over 933 Sentences.\n"
     ]
    }
   ],
   "source": [
    "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "train_auc_all= []\n",
    "train_sk_auc_all= []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "\n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "\n",
    "    #if (ix + 1) % 10 == 0:\n",
    "        #print('Training Observation #' + str(ix + 1) + ': Loss = ' + str(temp_loss))\n",
    "\n",
    "    # Keep trailing average of past 50 observations accuracy\n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data: t, y_target: y_data})\n",
    "    auc = sess.run(label_auc, feed_dict={x_data: t, y_target: y_data})\n",
    "    #sk_auc = metrics.roc_auc_score(y_data,temp_pred)\n",
    "    # Get True/False if prediction is accurate\n",
    "    train_acc_temp = target_train[ix] == np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    train_auc_all.append(auc)\n",
    "\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Test Set Accuracy For 400 Sentences.\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "\n",
      "Overall Test Accuracy: 0.87\n",
      "\n",
      "Overall Training auc: 0.3508424460887909\n",
      "\n",
      "Overall Test auc: 0.5201953053474426\n"
     ]
    }
   ],
   "source": [
    "# Get test set accuracy\n",
    "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\n",
    "test_acc_all = []\n",
    "test_auc_all =[]\n",
    "test_sk_auc_all =[]\n",
    "test_acc_avg = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    if (ix + 1) % 50 == 0:\n",
    "        print('Test Observation #' + str(ix + 1))\n",
    "\n",
    "        # Keep trailing average of past 50 observations accuracy\n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data: t, y_target: y_data})\n",
    "    auc = sess.run(label_auc, feed_dict={x_data: t, y_target: y_data})\n",
    "    # Get True/False if prediction is accurate\n",
    "    test_acc_temp = target_test[ix] == np.round(temp_pred)\n",
    "    test_auc_all.append(auc)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "    if len(test_acc_all) >= 50:\n",
    "        test_acc_avg.append(np.mean(test_acc_all[-50:]))\n",
    "print('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))\n",
    "\n",
    "print('\\nOverall Training auc: {}'.format(np.mean(train_auc_all)))\n",
    "print('\\nOverall Test auc: {}'.format(np.mean(test_auc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tf.ones(shape=[embedding_size,len(texts_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
