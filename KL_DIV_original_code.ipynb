{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "import re, math, collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kldiv(_s, _t):\n",
    "    if (len(_s) == 0):\n",
    "        return 1e33\n",
    " \n",
    "    if (len(_t) == 0):\n",
    "        return 1e33\n",
    " \n",
    "    ssum = 0. + sum(_s.values())\n",
    "    slen = len(_s)\n",
    " \n",
    "    tsum = 0. + sum(_t.values())\n",
    "    tlen = len(_t)\n",
    " \n",
    "    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n",
    "    lenvocabdiff = len(vocabdiff)\n",
    " \n",
    "    \"\"\" epsilon \"\"\"\n",
    "    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n",
    " \n",
    "    \"\"\" gamma \"\"\"\n",
    "    gamma = 1 - lenvocabdiff * epsilon\n",
    " \n",
    "    # print \"_s: %s\" % _s\n",
    "    # print \"_t: %s\" % _t\n",
    " \n",
    "    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n",
    "    sc = sum([v/ssum for v in _s.itervalues()])\n",
    "    st = sum([v/tsum for v in _t.itervalues()])\n",
    " \n",
    "    if sc < 9e-6:\n",
    "        print (\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print (\"*** ERROR: sc does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    "    if st < 9e-6:\n",
    "        print (\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print (\"*** ERROR: st does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    " \n",
    "    div = 0.\n",
    "    for t, v in _s.iteritems():\n",
    "        pts = v / ssum\n",
    " \n",
    "        ptt = epsilon\n",
    "        if t in _t:\n",
    "            ptt = gamma * (_t[t] / tsum)\n",
    " \n",
    "        ckl = (pts - ptt) * math.log(pts / ptt)\n",
    " \n",
    "        div +=  ckl\n",
    " \n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(lst):\n",
    "    prccd_item_list=[]\n",
    "    for tweet in lst:\n",
    "        # Normalizing utf8 formatting\n",
    "        tweet = tweet.decode(\"unicode-escape\").encode(\"utf8\").decode(\"utf8\")\n",
    "        tweet = tweet.encode(\"ascii\",\"ignore\")\n",
    "        tweet = tweet.strip('\\t\\n\\r')\n",
    "        # 1. Lowercasing\n",
    "        tweet = tweet.lower()\n",
    "        # Word-Level\n",
    "        tweet = re.sub(' +',' ',tweet) # replace multiple spaces with a single space\n",
    "        #  2. Normalizing digits\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word.isdigit()]:\n",
    "            tweet = tweet.replace(word, \"D\" * len(word))\n",
    "        # 3. Normalizing URLs\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if '/' in word or '.' in word and  len(word) > 3]:\n",
    "            tweet = tweet.replace(word, \"httpAddress\")\n",
    "        # 4. Normalizing username\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word[0] == '@' and len(word) > 1]:\n",
    "            tweet = tweet.replace(word, \"usrId\")\n",
    "        # 5. Removing special Characters\n",
    "        punc = '@$%^&*()_+-={}[]:\"|\\'\\~`<>/,'\n",
    "        trans = string.maketrans(punc, ' '*len(punc))\n",
    "        tweet = tweet.translate(trans)\n",
    "        # 6. Normalizing +2 elongated char\n",
    "        tweet = re.sub(r\"(.)\\1\\1+\",r'\\1\\1', tweet.decode('utf-8'))\n",
    "        #print(\"[elong]\", tweet)\n",
    "        # 7. tokenization using tweetNLP\n",
    "        tweet = ' '.join(simpleTokenize(tweet))\n",
    "        #8. fix \\n char\n",
    "        tweet = tweet.replace('\\n', ' ')\n",
    "\n",
    "        prccd_item_list.append(tweet.strip())\n",
    "    return prccd_item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(_str):\n",
    "    stopwords = ['and', 'for', 'if', 'the', 'then', 'be', 'is', 'are', 'will', 'in', 'it', 'to', 'that']\n",
    "    unwanted_words = [\"httpaddress\", \"usrid\", \"dd\", \"rt\", \"amp\", \"pm\", \" \", \"'s\", \"n't\", \"\\t\", '``', \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\"]\n",
    "    tokens = collections.defaultdict(lambda: 0.)\n",
    "    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n",
    "        m = m.group(1).lower()\n",
    "        if len(m) < 2: continue\n",
    "        if m in stopwords : continue\n",
    "        tokens[m] += 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = codecs.open('Data/amazon/books/processed/neg_books.txt', 'r', 'utf-8',errors='ignore')\n",
    "neg_books_doc_lst = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = codecs.open('Data/amazon/books/processed/pos_books.txt', 'r', 'utf-8',errors='ignore')\n",
    "pos_books_doc_lst = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f045fdcdd70>, {u'slowly': 1.0, u'imbibe': 1.0, u'certain': 1.0, u'one': 1.0, u'clear': 1.0, u'anger': 1.0, u'children': 1.0, u'needs': 1.0, u'entities': 1.0, u'perhaps': 1.0, u'interesting': 1.0, u'readable': 1.0, u'heady': 1.0, u'its': 2.0, u'religion': 1.0, u'book': 1.0, u'various': 1.0, u'pretty': 1.0, u'speech': 1.0, u'you': 1.0, u'reading': 1.0, u'ofcourse': 1.0, u'very': 1.0, u'but': 1.0, u'thoughts': 2.0, u'cool': 1.0, u'on': 1.0, u'about': 1.0, u'has': 2.0, u'like': 1.0, u'of': 1.0, u'consumed': 1.0, u'author': 1.0, u'thin': 1.0, u'things': 1.0, u'leaves': 1.0, u'revisited': 1.0, u'page': 1.0, u'silence': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(neg_books_doc_lst[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f045fdcd668>, {u'inspiring': 1.0, u'help': 1.0, u'morals': 1.0, u'question': 1.0, u'who': 1.0, u'allows': 1.0, u'discover': 1.0, u'book': 1.0, u'spiritually': 1.0, u'you': 3.0, u'your': 1.0, u'mentally': 1.0, u'really': 1.0})\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(pos_books_doc_lst[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean KL- div between books pos samples ', 3e+29)\n"
     ]
    }
   ],
   "source": [
    "books_neg_f_dist_dict_sample1 = random.sample(neg_books_doc_lst, 10000)\n",
    "books_pos_f_dist_dict_sample2 = random.sample(pos_books_doc_lst, 10000)\n",
    "\n",
    "KL_DIV_scores_books_pos_neg_samples = []\n",
    "\n",
    "for i in range(len(books_neg_f_dist_dict_sample1)):\n",
    "    KL_DIV_scores_books_pos_neg_samples.append(kldiv(tokenize(books_neg_f_dist_dict_sample1[i])\n",
    "                                                 ,tokenize(books_pos_f_dist_dict_sample2[i])))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_books_pos_neg_samples)/len(KL_DIV_scores_books_pos_neg_samples)\n",
    "print(\"mean KL- div between books pos samples \",KL_DIV_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
