{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "import re, math, collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load twokenize.py\n",
    "\"\"\"\n",
    "Twokenize -- a tokenizer designed for Twitter text in English and some other European languages.\n",
    "This tokenizer code has gone through a long history:\n",
    "\n",
    "(1) Brendan O'Connor wrote original version in Python, http://github.com/brendano/tweetmotif\n",
    "       TweetMotif: Exploratory Search and Topic Summarization for Twitter.\n",
    "       Brendan O'Connor, Michel Krieger, and David Ahn.\n",
    "       ICWSM-2010 (demo track), http://brenocon.com/oconnor_krieger_ahn.icwsm2010.tweetmotif.pdf\n",
    "(2a) Kevin Gimpel and Daniel Mills modified it for POS tagging for the CMU ARK Twitter POS Tagger\n",
    "(2b) Jason Baldridge and David Snyder ported it to Scala\n",
    "(3) Brendan bugfixed the Scala port and merged with POS-specific changes\n",
    "    for the CMU ARK Twitter POS Tagger  \n",
    "(4) Tobi Owoputi ported it back to Java and added many improvements (2012-06)\n",
    "\n",
    "Current home is http://github.com/brendano/ark-tweet-nlp and http://www.ark.cs.cmu.edu/TweetNLP\n",
    "\n",
    "There have been at least 2 other Java ports, but they are not in the lineage for the code here.\n",
    "\n",
    "Ported to Python by Myle Ott <myleott@gmail.com>.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import HTMLParser\n",
    "\n",
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "Contractions = re.compile(u\"(?i)(\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|['’′]re|['’′]s|['’′]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "\n",
    "punctChars = r\"['\\\"“”‘’.?!…,:;]\"\n",
    "#punctSeq   = punctChars+\"+\"\t#'anthem'. => ' anthem '.\n",
    "punctSeq   = r\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\t#'anthem'. => ' anthem ' .\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\"\n",
    "#  URLs\n",
    "\n",
    "\n",
    "# BTO 2012-06: everyone thinks the daringfireball regex should be better, but they're wrong.\n",
    "# If you actually empirically test it the results are bad.\n",
    "# Please see https://github.com/brendano/ark-tweet-nlp/pull/9\n",
    "\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs\t = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf_basic_trials|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\"\n",
    "\n",
    "\n",
    "# Numeric\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "#numNum     = r\"\\d+\\.\\d+\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb\t = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\\\\d+(?:\\\\.\\\\d+)+%?\".encode('utf-8')\n",
    "\n",
    "# Abbreviations\n",
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[“\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|―|—|~|–|=)\"\n",
    "decorations = u\"(?:[♫♪]+|[★☆]+|[♥❤♡]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\".encode('utf-8')\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['’′]\" + thingsThatSplitWords + \"*\"\n",
    "\n",
    "#  Emoticons\n",
    "# myleott: in Python the (?iu) flags affect the whole expression\n",
    "#normalEyes = \"(?iu)[:=]\" # 8 and x are eyes but cause problems\n",
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get :'-(\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(♥|0|o|°|v|\\\\$|t|x|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(♥|0|[oO]|°|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\".encode('utf-8')\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[＼\\\\ƪԄ\\(（<>;ヽ\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<>ʃ）/／ノﾉ丿╯σっµ~\\\\*]+\".encode('utf-8')\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]  \n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-―—=]*>+|<+[-―—=]*>*)\", u\"[\\u2190-\\u21ff]+\".encode('utf-8'))\n",
    "\n",
    "# BTO 2011-06: restored Hashtag, AtMention protection (dropped in original scala port) because it fixes\n",
    "# \"hello (#hashtag)\" ==> \"hello (#hashtag )\"  WRONG\n",
    "# \"hello (#hashtag)\" ==> \"hello ( #hashtag )\"  RIGHT\n",
    "# \"hello (@person)\" ==> \"hello (@person )\"  WRONG\n",
    "# \"hello (@person)\" ==> \"hello ( @person )\"  RIGHT\n",
    "# ... Some sort of weird interaction with edgepunct I guess, because edgepunct \n",
    "# has poor content-symbol detection.\n",
    "\n",
    "# This also gets #1 #40 which probably aren't hashtags .. but good as tokens.\n",
    "# If you want good hashtag identification, use a different regex.\n",
    "Hashtag = \"#[a-zA-Z0-9_]+\"  #optional: lookbehind for \\b\n",
    "#optional: lookbehind for \\b, max length 15\n",
    "AtMention = \"[@＠][a-zA-Z0-9_]+\"\n",
    "\n",
    "# I was worried this would conflict with at-mentions\n",
    "# but seems ok in sample of 5800: 7 changes all email fixes\n",
    "# http://www.regular-expressions.info/email.html\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\"\n",
    "\n",
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    unicode(regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        #numNum,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,  \n",
    "        AtMention\n",
    "    ).decode('utf-8')), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.  \n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"“”‘’«»{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n",
    "\n",
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "    \n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like \n",
    "    #     List((2,5), (8,10)) \n",
    "    # to create \n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "    \n",
    "    return zippedStr\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = HTMLParser.HTMLParser().unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "# \n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(lst):\n",
    "    prccd_item_list=[]\n",
    "    for tweet in lst:\n",
    "        # Normalizing utf8 formatting\n",
    "        tweet = tweet.decode(\"unicode-escape\").encode(\"utf8\").decode(\"utf8\")\n",
    "        tweet = tweet.encode(\"ascii\",\"ignore\")\n",
    "        tweet = tweet.strip('\\t\\n\\r')\n",
    "        # 1. Lowercasing\n",
    "        tweet = tweet.lower()\n",
    "        # Word-Level\n",
    "        tweet = re.sub(' +',' ',tweet) # replace multiple spaces with a single space\n",
    "        #  2. Normalizing digits\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word.isdigit()]:\n",
    "            tweet = tweet.replace(word, \"D\" * len(word))\n",
    "        # 3. Normalizing URLs\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if '/' in word or '.' in word and  len(word) > 3]:\n",
    "            tweet = tweet.replace(word, \"httpAddress\")\n",
    "        # 4. Normalizing username\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word[0] == '@' and len(word) > 1]:\n",
    "            tweet = tweet.replace(word, \"usrId\")\n",
    "        # 5. Removing special Characters\n",
    "        punc = '@$%^&*()_+-={}[]:\"|\\'\\~`<>/,'\n",
    "        trans = string.maketrans(punc, ' '*len(punc))\n",
    "        tweet = tweet.translate(trans)\n",
    "        # 6. Normalizing +2 elongated char\n",
    "        tweet = re.sub(r\"(.)\\1\\1+\",r'\\1\\1', tweet.decode('utf-8'))\n",
    "        #print(\"[elong]\", tweet)\n",
    "        # 7. tokenization using tweetNLP\n",
    "        tweet = ' '.join(simpleTokenize(tweet))\n",
    "        #8. fix \\n char\n",
    "        tweet = tweet.replace('\\n', ' ')\n",
    "\n",
    "        prccd_item_list.append(tweet.strip())\n",
    "    return prccd_item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generat_word_dist_bigrams(filename):\n",
    "    \n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    fp = codecs.open(filename, 'r', 'utf-8',errors='ignore')\n",
    "\n",
    "    words = nltk.word_tokenize(fp.read())\n",
    "\n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Stemming words seems to make matters worse, disabled\n",
    "    # stemmer = nltk.stem.snowball.SnowballStemmer('german')\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in default_stopwords]\n",
    "    \n",
    "    words = [clean_str(word) for word in words]\n",
    "    \n",
    "    words = nltk.bigrams(words)\n",
    "\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist\n",
    "    # Output top 50 words\n",
    "\n",
    "    #for word, frequency in fdist.most_common(50):\n",
    "        #print(u'{};{}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_tweets (filename):\n",
    "    with open(filename) as f:\n",
    "        content = list(line for line in (l.strip() for l in f) if line)\n",
    "        # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    tweets = [x.strip() for x in content] \n",
    "    processed_tweets = process(tweets)\n",
    "    return processed_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_words_tweets_dis(tweet):\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    unwanted_words = [\"httpaddress\", \"usrid\", \"dd\", \"rt\", \"amp\", \"pm\", \" \", \"'s\", \"n't\", \"\\t\", '``', \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\"]\n",
    "\n",
    "    \n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    words = nltk.word_tokenize(tweet)\n",
    "\n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Stemming words seems to make matters worse, disabled\n",
    "    # stemmer = nltk.stem.snowball.SnowballStemmer('german')\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in default_stopwords]\n",
    "    \n",
    "    words = [clean_str(word) for word in words]\n",
    "    \n",
    "    words = [word for word in words if word not in unwanted_words]\n",
    "\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generat_word_dist(filename):\n",
    "    \n",
    "    unwanted_words = [\"'s\",\" \",\"n't\",\"\\t\", '``', \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\"]\n",
    "    \n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    fp = codecs.open(filename, 'r', 'utf-8',errors='ignore')\n",
    "    \n",
    "    words = nltk.word_tokenize(fp.read())\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Stemming words seems to make matters worse, disabled\n",
    "    # stemmer = nltk.stem.snowball.SnowballStemmer('german')\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in default_stopwords]\n",
    "    \n",
    "    words = [clean_str(word) for word in words]\n",
    "    \n",
    "    words = [word for word in words if word not in unwanted_words]\n",
    "    \n",
    "    print(len(words))    \n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generat_word_dist_for_doc(document):\n",
    "    \n",
    "    unwanted_words = [\"'s\",\" \",\"n't\",\"\\t\", '``', \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\"]\n",
    "    \n",
    "    default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    words = nltk.word_tokenize(document)\n",
    "   \n",
    "    \n",
    "    #words = [unicode(word,'utf-8') for word in words]\n",
    "    \n",
    "    # Remove single-character tokens (mostly punctuation)\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    # Remove numbers\n",
    "    words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Stemming words seems to make matters worse, disabled\n",
    "    # stemmer = nltk.stem.snowball.SnowballStemmer('german')\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in default_stopwords]\n",
    "    \n",
    "    words = [clean_str(word) for word in words]\n",
    "    \n",
    "    words = [word for word in words if word not in unwanted_words]\n",
    "    # Calculate frequency distribution\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kldiv(_s, _t):\n",
    "    if (len(_s) == 0):\n",
    "        return 1e33\n",
    " \n",
    "    if (len(_t) == 0):\n",
    "        return 1e33\n",
    " \n",
    "    ssum = 0. + sum(_s.values())\n",
    "    slen = len(_s)\n",
    " \n",
    "    tsum = 0. + sum(_t.values())\n",
    "    tlen = len(_t)\n",
    " \n",
    "    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n",
    "    lenvocabdiff = len(vocabdiff)\n",
    " \n",
    "    \"\"\" epsilon \"\"\"\n",
    "    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n",
    " \n",
    "    \"\"\" gamma \"\"\"\n",
    "    gamma = 1 - lenvocabdiff * epsilon\n",
    " \n",
    "    # print \"_s: %s\" % _s\n",
    "    # print \"_t: %s\" % _t\n",
    " \n",
    "    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n",
    "    sc = sum([v/ssum for v in _s.itervalues()])\n",
    "    st = sum([v/tsum for v in _t.itervalues()])\n",
    " \n",
    "    if sc < 9e-6:\n",
    "        print (\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print (\"*** ERROR: sc does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    "    if st < 9e-6:\n",
    "        print (\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print (\"*** ERROR: st does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    " \n",
    "    div = 0.\n",
    "    for t, v in _s.iteritems():\n",
    "        pts = v / ssum\n",
    " \n",
    "        ptt = epsilon\n",
    "        if t in _t:\n",
    "            ptt = gamma * (_t[t] / tsum)\n",
    " \n",
    "        ckl = (pts - ptt) * math.log(pts / ptt)\n",
    " \n",
    "        div +=  ckl\n",
    " \n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in PV dataset\n",
    "fp = codecs.open('Data/All_Pos_PV_DS.text', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "Pv_doc_word_freq_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({u'affiliated': 1,\n",
       "          u'beni': 1,\n",
       "          u'day': 1,\n",
       "          u'group': 1,\n",
       "          u'members': 1,\n",
       "          u'monday': 1,\n",
       "          u'oulbane': 1,\n",
       "          u'protest': 2,\n",
       "          u'protesters': 1,\n",
       "          u'skikda': 1,\n",
       "          u'staged': 2,\n",
       "          u'unpef': 2})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pv_doc_word_freq_dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in reuters violence negative dataset\n",
    "fp = codecs.open('Data/reuters_negative_path.txt', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "reuters_f_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in books neg reviews\n",
    "fp = codecs.open('Data/amazon/books/processed/neg_books.txt', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "books_neg_f_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in books po reviews\n",
    "fp = codecs.open('Data/amazon/books/processed/pos_books.txt', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "books_pos_f_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in movies neg reviews\n",
    "fp = codecs.open('Data/amazon/movies/processed/neg_movies.txt', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "movies_neg_f_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in movies pos reviews\n",
    "fp = codecs.open('Data/amazon/movies/processed/pos_movies.txt', 'r', 'utf-8',errors='ignore')\n",
    "doc_lst = fp.readlines()\n",
    "movies_pos_f_dist = map (generat_word_dist_for_doc, doc_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in turkish tweets dataset\n",
    "turkish_tweets = process_tweets('Data/turkish_protests_unduplicated_100000_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turkish_tweets = process_tweets('Data/turkish_protests_unduplicated_100000_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word freq distribution in unlabeled turkish tweets\n",
    "turkish_tweets_f_dist = map(generate_words_tweets_dis, turkish_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF_pos_tweets = process_tweets('Data/turkish_protest_test_pos_prccd2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_pos_tweets_f_dist = map (generate_words_tweets_dis, CF_pos_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF_neg_tweets = process_tweets('Data/turkish_protest_test_neg_prccd2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_neg_tweets_f_dist = map(generate_words_tweets_dis, CF_neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_word_frequencies_file(freq_dist, filePath):\n",
    "    term_dict = {}\n",
    "    with open(filePath,'w') as f:\n",
    "        for word, frequency in freq_dist.most_common(1000000):\n",
    "            f.write(u'{} {}'.format(word, frequency)+ '\\n')\n",
    "            term_dict[word] = frequency\n",
    "    return term_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_word_frequencies(freq_dist):\n",
    "    term_dict = {}\n",
    "    for word, frequency in freq_dist.most_common(1000000):\n",
    "        term_dict[word] = frequency\n",
    "    return term_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dict (x , y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pv_f_dist_dict = map(print_word_frequencies, Pv_doc_word_freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turkish_tweets_f_dist_dict = map(print_word_frequencies, turkish_tweets_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuters_f_dist_dict = map(print_word_frequencies, reuters_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books_pos_f_dist_dict = map(print_word_frequencies, books_pos_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books_neg_f_dist_dict = map(print_word_frequencies, books_neg_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'1 2': 1,\n",
       " u'anger': 1,\n",
       " u'author': 1,\n",
       " u'book': 1,\n",
       " u'certain': 1,\n",
       " u'children': 1,\n",
       " u'clear': 1,\n",
       " u'consumed': 1,\n",
       " u'cool': 1,\n",
       " u'entities': 1,\n",
       " u'heady': 1,\n",
       " u'imbibe': 1,\n",
       " u'interesting': 1,\n",
       " u'leaves': 1,\n",
       " u'like': 1,\n",
       " u'needs': 1,\n",
       " u'ofcourse': 1,\n",
       " u'one': 1,\n",
       " u'page': 1,\n",
       " u'perhaps': 1,\n",
       " u'pretty': 1,\n",
       " u'readable': 1,\n",
       " u'reading': 1,\n",
       " u'religion': 1,\n",
       " u'revisited': 1,\n",
       " u'silence': 1,\n",
       " u'slowly': 1,\n",
       " u'speech': 1,\n",
       " u'thin': 1,\n",
       " u'things': 1,\n",
       " u'thoughts': 2,\n",
       " u'various': 1}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_neg_f_dist_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_neg_f_dist_dict = map(print_word_frequencies, movies_neg_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_pos_f_dist_dict = map(print_word_frequencies, movies_pos_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF_pos_tweets_f_dist_dict = map(print_word_frequencies, CF_pos_tweets_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF_neg_tweets_f_dist_dict = map(print_word_frequencies, CF_neg_tweets_f_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between PV samples 0.0\n"
     ]
    }
   ],
   "source": [
    "KL_DIV_scores_PV_sample = []\n",
    "\n",
    "for i in range(len(pv_f_dist_dict)):\n",
    "    KL_DIV_scores_PV_sample.append(kldiv(pv_f_dist_dict[i],pv_f_dist_dict[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_PV_sample)/len(KL_DIV_scores_PV_sample)\n",
    "print(\"mean KL- div between PV samples\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-276-09dbf4296e18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpv_f_dist_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encode' is not defined"
     ]
    }
   ],
   "source": [
    "print(encode(pv_f_dist_dict[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between PV samples 7.10225080582\n"
     ]
    }
   ],
   "source": [
    "pv_f_dist_dict_sample1 = random.sample(pv_f_dist_dict[0],25000)\n",
    "pv_f_dist_dict_sample2 = random.sample(pv_f_dist_dict,25000)\n",
    "\n",
    "KL_DIV_scores_PV_samples = []\n",
    "\n",
    "for i in range(len(pv_f_dist_dict_sample1)):\n",
    "    KL_DIV_scores_PV_samples.append(kldiv(pv_f_dist_dict_sample1[i],pv_f_dist_dict_sample2[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_PV_samples)/len(KL_DIV_scores_PV_samples)\n",
    "print(\"mean KL- div between PV samples\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between reuters samples 6.85191514129\n"
     ]
    }
   ],
   "source": [
    "reuters_f_dist_dict_sample1 = random.sample(reuters_f_dist_dict,25000)\n",
    "reuters_f_dist_dict_sample2 = random.sample(reuters_f_dist_dict,25000)\n",
    "\n",
    "KL_DIV_scores_reuters_samples = []\n",
    "\n",
    "for i in range(len(reuters_f_dist_dict_sample1)):\n",
    "    KL_DIV_scores_reuters_samples.append(kldiv(reuters_f_dist_dict_sample1[i],reuters_f_dist_dict_sample2[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_reuters_samples)/len(KL_DIV_scores_reuters_samples)\n",
    "print(\"mean KL- div between reuters samples\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between tweets samples 2.848e+31\n"
     ]
    }
   ],
   "source": [
    "KL_DIV_scores_tweets_sample1 = random.sample(turkish_tweets_f_dist_dict,25000)\n",
    "KL_DIV_scores_tweets_sample2 = random.sample(turkish_tweets_f_dist_dict,25000)\n",
    "\n",
    "KL_DIV_scores_tweets_samples = []\n",
    "\n",
    "for i in range(len(KL_DIV_scores_tweets_sample1)):\n",
    "    KL_DIV_scores_tweets_samples.append(kldiv(KL_DIV_scores_tweets_sample1[i],KL_DIV_scores_tweets_sample2[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_tweets_samples)/len(KL_DIV_scores_tweets_samples)\n",
    "print(\"mean KL- div between tweets samples\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between PV DS (positive examples) and Reuters (negative examples) 7.01718673072\n"
     ]
    }
   ],
   "source": [
    "pv_f_dist_dict_samples = random.sample(pv_f_dist_dict,25000)\n",
    "KL_DIV_scores_PV_reuters = []\n",
    "\n",
    "for i in range(len(pv_f_dist_dict_samples)):\n",
    "    KL_DIV_scores_PV_reuters.append(kldiv(pv_f_dist_dict_samples[i],reuters_f_dist_dict[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_PV_reuters)/len(KL_DIV_scores_PV_reuters)\n",
    "print(\"mean KL- div between PV DS (positive examples) and Reuters (negative examples)\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean KL-DIV between PV (labeled) and tweets (unlabeled): 1.432e+31\n"
     ]
    }
   ],
   "source": [
    "pv_f_dist_dict_samples = random.sample(pv_f_dist_dict,25000)\n",
    "turkish_tweets_f_dist_dict_sample = random.sample(turkish_tweets_f_dist_dict, 25000)\n",
    "\n",
    "KL_DIV_scores_PV_tweets = []\n",
    "\n",
    "for i in range(len(pv_f_dist_dict_samples)):\n",
    "    KL_DIV_scores_PV_tweets.append(kldiv(pv_f_dist_dict_samples[i],turkish_tweets_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_PV_tweets)/len(KL_DIV_scores_PV_tweets)\n",
    "print(\" mean KL-DIV between PV (labeled) and tweets (unlabeled):\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between tweets and Reuters (negative examples) 1.536e+31\n"
     ]
    }
   ],
   "source": [
    "turkish_tweets_f_dist_dict_sample = random.sample(turkish_tweets_f_dist_dict, 25000)\n",
    "\n",
    "KL_DIV_scores_tweets_reuters = []\n",
    "\n",
    "for i in range(len(turkish_tweets_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_tweets_reuters.append(kldiv(turkish_tweets_f_dist_dict_sample[i],reuters_f_dist_dict[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_tweets_reuters)/len(KL_DIV_scores_tweets_reuters)\n",
    "print(\"mean KL- div between tweets and Reuters (negative examples)\",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between books pos and neg  7e+29\n"
     ]
    }
   ],
   "source": [
    "books_pos_f_dist_dict_sample = random.sample(books_pos_f_dist_dict, 10000)\n",
    "books_neg_f_dist_dict_sample = random.sample(books_neg_f_dist_dict, 10000)\n",
    "\n",
    "KL_DIV_scores_books = []\n",
    "\n",
    "for i in range(len(books_pos_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_books.append(kldiv(books_pos_f_dist_dict_sample[i],books_neg_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_books)/len(KL_DIV_scores_books)\n",
    "print(\"mean KL- div between books pos and neg \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between movies pos and neg  4.23504499735e+29\n"
     ]
    }
   ],
   "source": [
    "movies_pos_f_dist_dict_sample = random.sample(movies_pos_f_dist_dict, 9445)\n",
    "movies_neg_f_dist_dict_sample = random.sample(movies_neg_f_dist_dict, 9445)\n",
    "\n",
    "KL_DIV_scores_movies = []\n",
    "\n",
    "for i in range(len(movies_pos_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_movies.append(kldiv(movies_pos_f_dist_dict_sample[i],movies_neg_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_movies)/len(KL_DIV_scores_movies)\n",
    "print(\"mean KL- div between movies pos and neg \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between books pos samples  4e+29\n"
     ]
    }
   ],
   "source": [
    "books_pos_f_dist_dict_sample1 = random.sample(books_pos_f_dist_dict, 10000)\n",
    "books_pos_f_dist_dict_sample2 = random.sample(books_pos_f_dist_dict, 10000)\n",
    "\n",
    "KL_DIV_scores_books_pos_samples = []\n",
    "\n",
    "for i in range(len(books_pos_f_dist_dict_sample1)):\n",
    "    KL_DIV_scores_books_pos_samples.append(kldiv(books_pos_f_dist_dict_sample1[i],books_pos_f_dist_dict_sample2[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_books_pos_samples)/len(KL_DIV_scores_books_pos_samples)\n",
    "print(\"mean KL- div between books pos samples \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between movies pos samples  1e+29\n"
     ]
    }
   ],
   "source": [
    "movies_pos_f_dist_dict_sample1 = random.sample(movies_pos_f_dist_dict, 10000)\n",
    "movies_pos_f_dist_dict_sample2 = random.sample(movies_pos_f_dist_dict, 10000)\n",
    "\n",
    "KL_DIV_scores_movies_pos_samples = []\n",
    "\n",
    "for i in range(len(movies_pos_f_dist_dict_sample1)):\n",
    "    KL_DIV_scores_movies_pos_samples.append(kldiv(movies_pos_f_dist_dict_sample1[i],movies_pos_f_dist_dict_sample2[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_movies_pos_samples)/len(KL_DIV_scores_movies_pos_samples)\n",
    "print(\"mean KL- div between movies pos samples \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between books and movies  1e+29\n"
     ]
    }
   ],
   "source": [
    "books_pos_f_dist_dict_sample = random.sample(books_pos_f_dist_dict, 10000)\n",
    "movies_pos_f_dist_dict_sample = random.sample(movies_pos_f_dist_dict, 10000)\n",
    "\n",
    "KL_DIV_scores_books_movies = []\n",
    "\n",
    "for i in range(len(books_pos_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_books_movies.append(kldiv(books_pos_f_dist_dict_sample[i],movies_pos_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_books_movies)/len(KL_DIV_scores_books_movies)\n",
    "print(\"mean KL- div between pos books and movies \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between pos books and movies  5.29380624669e+29\n"
     ]
    }
   ],
   "source": [
    "books_neg_f_dist_dict_sample = random.sample(books_neg_f_dist_dict, 9445)\n",
    "movies_neg_f_dist_dict_sample = random.sample(movies_neg_f_dist_dict, 9445)\n",
    "\n",
    "KL_DIV_scores_books_movies_neg = []\n",
    "\n",
    "for i in range(len(books_neg_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_books_movies_neg.append(kldiv(books_neg_f_dist_dict_sample[i],movies_neg_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_books_movies_neg)/len(KL_DIV_scores_books_movies_neg)\n",
    "print(\"mean KL- div between neg books and movies \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean KL- div between CF tweets pos and neg  6.78159870939\n"
     ]
    }
   ],
   "source": [
    "CF_tweets_pos_f_dist_dict_sample = random.sample(CF_pos_tweets_f_dist_dict, 54)\n",
    "CF_tweets_neg_f_dist_dict_sample = random.sample(CF_neg_tweets_f_dist_dict, 54)\n",
    "\n",
    "KL_DIV_scores_CF_tweets = []\n",
    "\n",
    "for i in range(len(CF_tweets_pos_f_dist_dict_sample)):\n",
    "    KL_DIV_scores_CF_tweets.append(kldiv(CF_tweets_pos_f_dist_dict_sample[i],CF_tweets_neg_f_dist_dict_sample[i]))\n",
    "    \n",
    "KL_DIV_mean= reduce(lambda x, y: x+y,KL_DIV_scores_CF_tweets)/len(KL_DIV_scores_CF_tweets)\n",
    "print(\"mean KL- div between CF tweets pos and neg \",KL_DIV_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(_str):\n",
    "    stopwords = ['and', 'for', 'if', 'the', 'then', 'be', 'is', 'are', 'will', 'in', 'it', 'to', 'that']\n",
    "    tokens = collections.defaultdict(lambda: 0.)\n",
    "    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n",
    "        m = m.group(1).lower()\n",
    "        if len(m) < 2: continue\n",
    "        if m in stopwords: continue\n",
    "        tokens[m] += 1\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7f3d4e1318c0>, {'me': 1.0, 'kill': 1.0})\n",
      "KL-divergence between d1 and d2: 6.9008475237\n",
      "KL-divergence between d2 and d1: 6.9008475237\n"
     ]
    }
   ],
   "source": [
    "d1 = \"kill me\"\n",
    "d2 = \"money bank\"\n",
    "\n",
    "print(tokenize(d1))\n",
    "print (\"KL-divergence between d1 and d2:\", kldiv(tokenize(d1), tokenize(d2)))\n",
    "print (\"KL-divergence between d2 and d1:\", kldiv(tokenize(d2), tokenize(d1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_files = ['Data/All_Pos_PV_DS.text', 'Data/reuters_negative_path.txt','Data/turkish_protests_unduplicated_100000_text.txt']\n",
    "\n",
    "pv_file = codecs.open('Data/All_Pos_PV_DS.text', 'r', 'utf-8',errors='ignore').read()\n",
    "reuters_file = codecs.open('Data/reuters_negative_path.txt', 'r', 'utf-8',errors='ignore').read()\n",
    "pv_retures = pv_file + reuters_file\n",
    "tweets_file = codecs.open('Data/turkish_protests_unduplicated_100000_text.txt', 'r', 'utf-8',errors='ignore').read()\n",
    "\n",
    "documents= [pv_retures, tweets_file]\n",
    "tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "documents2= [pv_file, reuters_file, tweets_file]\n",
    "tfidf2 = TfidfVectorizer().fit_transform(documents2)\n",
    "pairwise_similarity2 = tfidf2 * tfidf2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf_tweets_pos = codecs.open('Data/turkish_protest_test_pos_prccd2.txt', 'r', 'utf-8',errors='ignore').read()\n",
    "cf_tweets_neg = codecs.open('Data/turkish_protest_test_neg_prccd2.txt', 'r', 'utf-8',errors='ignore').read()\n",
    "cf_tweets = cf_tweets_pos + cf_tweets_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF_tweets_docs= [cf_tweets_pos, cf_tweets_neg]\n",
    "tfidf_cf_tweets = TfidfVectorizer().fit_transform(CF_tweets_docs)\n",
    "cf_tweets_pairwise_similarity = tfidf_cf_tweets * tfidf_cf_tweets.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_target_test= [pv_retures, tweets_file, cf_tweets]\n",
    "tfidf_source_target_test = TfidfVectorizer().fit_transform(source_target_test)\n",
    "tfidf_source_target_test_pairwise_similarity = tfidf_source_target_test * tfidf_source_target_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.49462647],\n",
       "       [ 0.49462647,  1.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.58096961,  0.45686109],\n",
       "       [ 0.58096961,  1.        ,  0.32611657],\n",
       "       [ 0.45686109,  0.32611657,  1.        ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.89386843],\n",
       "       [ 0.89386843,  1.        ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_tweets_pairwise_similarity.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.47272487,  0.26941537],\n",
       "       [ 0.47272487,  1.        ,  0.35833355],\n",
       "       [ 0.26941537,  0.35833355,  1.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_source_target_test_pairwise_similarity.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
