{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fatma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fatma/anaconda2/envs/conda_python_env/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \", string)\n",
    "    string = re.sub(r\"n\\'t\", \" \", string)\n",
    "    string = re.sub(r\"n\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'re\", \" \", string)\n",
    "    string = re.sub(r\"\\'d\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \", string)\n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" \", string)\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\?\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\//?\", \" \", string)\n",
    "    string = re.sub(r\"\\d+\", \" \", string)\n",
    "    string = re.sub(r\"\\$\", \" \", string)\n",
    "    string = re.sub(r\"\\#\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(str):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    lst = str.split()\n",
    "    lst = [i for i in lst if i not in stop]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def stem (str):\n",
    "    lst = str.split()\n",
    "    lst = [st.stem(x) for x in lst]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def remove_unwanted_words(str):\n",
    "    unwanted_words = [\"httpaddress\", \"usrid\", \"D\", \"dd\", \"rt\", \"amp\", \"am\", \"pm\", '``',\n",
    "                      \"''\", \"\", \"//\", \"\\\\\", \"\\\\'s\", \"\\\\?\", \"\\?\",\"http\",\"httpaddresshttpaddresst\", \"cohttpaddressek\",\n",
    "                      \"taksim\",\"gezi\", \"park\", \"direngeziparki\", \"occupygezi\", \"istanbul\", \"turkish\",\"turkey\",\n",
    "                      \"protest\",\"direngezipark\",\"direnankara\",\"geziparki\", \"protesters\", \"protests\", \"sat\", \"sun\", \"mon,\",\n",
    "                     \"tue\", \"wed\", \"thu\", \"fri\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    lst = str.split(\" \")\n",
    "    lst = [i for i in lst if i not in unwanted_words]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def toLower(str):\n",
    "    lst = str.split()\n",
    "    lst = [i.lower() for i in lst]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def word_len (str):\n",
    "    lst = str.split()\n",
    "    lst = [i for i in lst if len(i)>1 and len(i) <7]\n",
    "    return ' '.join(lst)\n",
    "\n",
    "def sent_len (str):\n",
    "    lst = str.split()\n",
    "    if len(lst)>=3:\n",
    "        return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels_shuffled(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity african_data from files, splits the african_data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load african_data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    positive_examples = [remove_stop(item) for item in positive_examples]\n",
    "    #print(positive_examples[0])\n",
    "    positive_examples = [toLower(item) for item in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    positive_examples = [clean_str(sent) for sent in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    #positive_examples = [stem(item) for item in positive_examples]\n",
    "    #rint(positive_examples[0])\n",
    "    positive_examples = [remove_unwanted_words(item) for item in positive_examples]\n",
    "    positive_examples = [word_len(item) for item in positive_examples]\n",
    "    positive_examples = [sent_len(item) for item in positive_examples]\n",
    "    positive_examples = list(filter(None, positive_examples))\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    negative_examples = [toLower(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [remove_stop(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [clean_str(sent) for sent in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    #ngative_examples = [stem(sent) for sent in negative_examples]\n",
    "    #pint(negative_examples[0])\n",
    "    negative_examples = [remove_unwanted_words(item) for item in negative_examples]\n",
    "    negative_examples = [word_len(item) for item in negative_examples]\n",
    "    #print(negative_examples[0])\n",
    "    negative_examples = [sent_len(item) for item in negative_examples]\n",
    "\n",
    "    negative_examples = list(filter(None, negative_examples))\n",
    "\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    # x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, target):\n",
    "  \"\"\"A bag-of-words model. Note it disregards the word order in the text.\"\"\"\n",
    "  target = tf.one_hot(target, 2, 1, 0)\n",
    "  features = tf.contrib.layers.bow_encoder(\n",
    "      features, vocab_size=n_words, embed_dim=Embedding_size, scope=\"input_layer\")\n",
    "  hidden_layer1 = tf.contrib.layers.fully_connected(features, 100, scope=\"hidden_layer1\")\n",
    "  logits = tf.contrib.layers.fully_connected(hidden_layer1, 2, scope=\"output_layer\",\n",
    "      activation_fn=None)\n",
    "  loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss, tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam', learning_rate=0.01)\n",
    "  return (\n",
    "      {'class': tf.argmax(logits, 1),\n",
    "       'prob': tf.nn.softmax(logits)},\n",
    "      loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Choose max text word length at 25\n",
    "sentence_size = 10\n",
    "min_word_freq = 3\n",
    "Words_Features = 'words'\n",
    "Embedding_size = 50\n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_text, tweets_y = load_data_and_labels_shuffled('Data/Turkish_tweets_CF_results_09_05_2018_prccd_pos.txt', 'Data/Turkish_tweets_CF_results_09_05_2018_prccd_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size =  1333\n"
     ]
    }
   ],
   "source": [
    "# Setup vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size , min_frequency=min_word_freq)\n",
    "\n",
    "# Have to fit transform to get length of unique words.\n",
    "vocab_processor.transform(tweets_text)\n",
    "embedding_size = len([x for x in vocab_processor.transform(tweets_text)])\n",
    "print(\"embedding size = \", embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up data set into train/test\n",
    "train_indices = np.random.choice(len(tweets_text), round(len(tweets_text) * 0.7), replace=False)\n",
    "test_indices = np.array(list(set(range(len(tweets_text))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(tweets_text) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(tweets_text) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(tweets_y) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(tweets_y) if ix in test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process vocabulary\n",
    "texts_train = np.array(list(vocab_processor.fit_transform(texts_train)))\n",
    "texts_test = np.array(list(vocab_processor.transform(texts_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 316\n",
      "train size 933\n",
      "positive samples 129\n",
      "negative samples 804\n",
      "test size 400\n",
      "positive samples 51\n",
      "negative samples 349\n",
      "no. words 316\n"
     ]
    }
   ],
   "source": [
    "print('Total words: %d' % n_words)\n",
    "\n",
    "print(\"train size\", len(texts_train))\n",
    "print(\"positive samples\", target_train.count([1]))\n",
    "print(\"negative samples\", target_train.count([0]))\n",
    "\n",
    "print(\"test size\", len(texts_test))\n",
    "print(\"positive samples\",  target_test.count([1]))\n",
    "print(\"negative samples\", target_test.count([0]))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('no. words', n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 400\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(data, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\n",
    "\n",
    "    Given a list of array-like objects, generate batches of a given\n",
    "    size by yielding a list of array-like objects corresponding to the\n",
    "    same slice of each input.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        data = shuffle_aligned_list(data)\n",
    "\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size >= len(data[0]): #the tarining examples\n",
    "            batch_count = 0\n",
    "\n",
    "            if shuffle:\n",
    "                data = shuffle_aligned_list(data)\n",
    "\n",
    "        start = batch_count * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_count += 1\n",
    "        \n",
    "        yield [d[start:end] for d in data] #for the instances and then for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch = batch_generator([texts_train, target_train], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    for epoch in range(n_epoches):\n",
    "        for iteration in range(len(texts_train) // batch_size):\n",
    "            X_batch, y_batch = next(gen_batch)\n",
    "            sess.run(trainin_op, feed_dict={X: X_batch, y:y y_batch})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1234vp66\n",
      "INFO:tensorflow:Using config: {'_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_session_config': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_task_type': None, '_environment': 'local', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f91d070fc50>, '_keep_checkpoint_max': 5, '_is_chief': True, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_master': '', '_num_ps_replicas': 0, '_evaluation_master': '', '_task_id': 0, '_model_dir': '/tmp/tmp1234vp66', '_save_checkpoints_secs': 600, '_num_worker_replicas': 0}\n",
      "WARNING:tensorflow:From <ipython-input-13-db6a26d3bf25>:3: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-13-db6a26d3bf25>:3: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-8-8db02ad9525c>:9: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /home/fatma/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /home/fatma/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp1234vp66/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 0.71553004\n",
      "INFO:tensorflow:global_step/sec: 201.152\n",
      "INFO:tensorflow:step = 101, loss = 0.035153817 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.58\n",
      "INFO:tensorflow:step = 201, loss = 0.030976117 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.5\n",
      "INFO:tensorflow:step = 301, loss = 0.030523956 (0.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.307\n",
      "INFO:tensorflow:step = 401, loss = 0.030037116 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.415\n",
      "INFO:tensorflow:step = 501, loss = 0.029898403 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.816\n",
      "INFO:tensorflow:step = 601, loss = 0.02977065 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.814\n",
      "INFO:tensorflow:step = 701, loss = 0.029825903 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.521\n",
      "INFO:tensorflow:step = 801, loss = 0.030014904 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.204\n",
      "INFO:tensorflow:step = 901, loss = 0.029794114 (0.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.708\n",
      "INFO:tensorflow:step = 1001, loss = 0.02984176 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.997\n",
      "INFO:tensorflow:step = 1101, loss = 0.029814538 (0.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.685\n",
      "INFO:tensorflow:step = 1201, loss = 0.02977269 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.407\n",
      "INFO:tensorflow:step = 1301, loss = 0.02976397 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.266\n",
      "INFO:tensorflow:step = 1401, loss = 0.029713204 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.369\n",
      "INFO:tensorflow:step = 1501, loss = 0.029706163 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.103\n",
      "INFO:tensorflow:step = 1601, loss = 0.029724956 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.8\n",
      "INFO:tensorflow:step = 1701, loss = 0.02974548 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.409\n",
      "INFO:tensorflow:step = 1801, loss = 0.029955685 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.905\n",
      "INFO:tensorflow:step = 1901, loss = 0.02970983 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.147\n",
      "INFO:tensorflow:step = 2001, loss = 0.02987466 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.249\n",
      "INFO:tensorflow:step = 2101, loss = 0.029989855 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.154\n",
      "INFO:tensorflow:step = 2201, loss = 0.029788125 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.386\n",
      "INFO:tensorflow:step = 2301, loss = 0.029791223 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.257\n",
      "INFO:tensorflow:step = 2401, loss = 0.029804185 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.882\n",
      "INFO:tensorflow:step = 2501, loss = 0.029696504 (0.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.504\n",
      "INFO:tensorflow:step = 2601, loss = 0.029717255 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.501\n",
      "INFO:tensorflow:step = 2701, loss = 0.029709576 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.148\n",
      "INFO:tensorflow:step = 2801, loss = 0.029779775 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.788\n",
      "INFO:tensorflow:step = 2901, loss = 0.029726438 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.468\n",
      "INFO:tensorflow:step = 3001, loss = 0.029709399 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.562\n",
      "INFO:tensorflow:step = 3101, loss = 0.02994713 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.804\n",
      "INFO:tensorflow:step = 3201, loss = 0.029728912 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.763\n",
      "INFO:tensorflow:step = 3301, loss = 0.02989665 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.197\n",
      "INFO:tensorflow:step = 3401, loss = 0.029947747 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.094\n",
      "INFO:tensorflow:step = 3501, loss = 0.029844094 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.987\n",
      "INFO:tensorflow:step = 3601, loss = 0.029740745 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.476\n",
      "INFO:tensorflow:step = 3701, loss = 0.029786035 (0.509 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.694\n",
      "INFO:tensorflow:step = 3801, loss = 0.03016489 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.516\n",
      "INFO:tensorflow:step = 3901, loss = 0.029738449 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.082\n",
      "INFO:tensorflow:step = 4001, loss = 0.029750848 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.251\n",
      "INFO:tensorflow:step = 4101, loss = 0.029720278 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.117\n",
      "INFO:tensorflow:step = 4201, loss = 0.029708898 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.739\n",
      "INFO:tensorflow:step = 4301, loss = 0.02976479 (0.533 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.187\n",
      "INFO:tensorflow:step = 4401, loss = 0.030010797 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.867\n",
      "INFO:tensorflow:step = 4501, loss = 0.029806158 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.765\n",
      "INFO:tensorflow:step = 4601, loss = 0.029704822 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 187.393\n",
      "INFO:tensorflow:step = 4701, loss = 0.029813537 (0.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.774\n",
      "INFO:tensorflow:step = 4801, loss = 0.029700194 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.971\n",
      "INFO:tensorflow:step = 4901, loss = 0.02971254 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.414\n",
      "INFO:tensorflow:step = 5001, loss = 0.029706866 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.728\n",
      "INFO:tensorflow:step = 5101, loss = 0.029711302 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.481\n",
      "INFO:tensorflow:step = 5201, loss = 0.029688906 (0.514 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 191.321\n",
      "INFO:tensorflow:step = 5301, loss = 0.029727882 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.328\n",
      "INFO:tensorflow:step = 5401, loss = 0.029716458 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.199\n",
      "INFO:tensorflow:step = 5501, loss = 0.029690463 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.181\n",
      "INFO:tensorflow:step = 5601, loss = 0.029711075 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.979\n",
      "INFO:tensorflow:step = 5701, loss = 0.029687783 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.543\n",
      "INFO:tensorflow:step = 5801, loss = 0.029711979 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.017\n",
      "INFO:tensorflow:step = 5901, loss = 0.02976758 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.818\n",
      "INFO:tensorflow:step = 6001, loss = 0.029704448 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.729\n",
      "INFO:tensorflow:step = 6101, loss = 0.029690998 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.386\n",
      "INFO:tensorflow:step = 6201, loss = 0.029942164 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.564\n",
      "INFO:tensorflow:step = 6301, loss = 0.02971937 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.127\n",
      "INFO:tensorflow:step = 6401, loss = 0.029688843 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.423\n",
      "INFO:tensorflow:step = 6501, loss = 0.02973773 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.86\n",
      "INFO:tensorflow:step = 6601, loss = 0.029690702 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.289\n",
      "INFO:tensorflow:step = 6701, loss = 0.029709462 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.96\n",
      "INFO:tensorflow:step = 6801, loss = 0.02969164 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.847\n",
      "INFO:tensorflow:step = 6901, loss = 0.02972594 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.878\n",
      "INFO:tensorflow:step = 7001, loss = 0.029692845 (0.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.017\n",
      "INFO:tensorflow:step = 7101, loss = 0.029687233 (0.526 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.485\n",
      "INFO:tensorflow:step = 7201, loss = 0.029727977 (0.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.903\n",
      "INFO:tensorflow:step = 7301, loss = 0.02969618 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.891\n",
      "INFO:tensorflow:step = 7401, loss = 0.029721338 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.023\n",
      "INFO:tensorflow:step = 7501, loss = 0.029718636 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.558\n",
      "INFO:tensorflow:step = 7601, loss = 0.029730015 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.879\n",
      "INFO:tensorflow:step = 7701, loss = 0.029696802 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.643\n",
      "INFO:tensorflow:step = 7801, loss = 0.029925315 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.757\n",
      "INFO:tensorflow:step = 7901, loss = 0.029691093 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.511\n",
      "INFO:tensorflow:step = 8001, loss = 0.029750723 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.915\n",
      "INFO:tensorflow:step = 8101, loss = 0.029716592 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.871\n",
      "INFO:tensorflow:step = 8201, loss = 0.029718323 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.084\n",
      "INFO:tensorflow:step = 8301, loss = 0.02982166 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.008\n",
      "INFO:tensorflow:step = 8401, loss = 0.029694336 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.378\n",
      "INFO:tensorflow:step = 8501, loss = 0.029781934 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.289\n",
      "INFO:tensorflow:step = 8601, loss = 0.029697975 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.875\n",
      "INFO:tensorflow:step = 8701, loss = 0.029718362 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.58\n",
      "INFO:tensorflow:step = 8801, loss = 0.029715776 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.803\n",
      "INFO:tensorflow:step = 8901, loss = 0.029737256 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.032\n",
      "INFO:tensorflow:step = 9001, loss = 0.030050615 (0.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 177.425\n",
      "INFO:tensorflow:step = 9101, loss = 0.029688993 (0.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 176.933\n",
      "INFO:tensorflow:step = 9201, loss = 0.029691022 (0.565 sec)\n",
      "INFO:tensorflow:global_step/sec: 184.169\n",
      "INFO:tensorflow:step = 9301, loss = 0.029703593 (0.543 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.784\n",
      "INFO:tensorflow:step = 9401, loss = 0.029684305 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.855\n",
      "INFO:tensorflow:step = 9501, loss = 0.029770736 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.872\n",
      "INFO:tensorflow:step = 9601, loss = 0.029685177 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.854\n",
      "INFO:tensorflow:step = 9701, loss = 0.029689154 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.043\n",
      "INFO:tensorflow:step = 9801, loss = 0.029710457 (0.515 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.012\n",
      "INFO:tensorflow:step = 9901, loss = 0.029993562 (0.513 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmp1234vp66/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.029688722.\n",
      "WARNING:tensorflow:From <ipython-input-13-db6a26d3bf25>:4: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-8-8db02ad9525c>:9: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /home/fatma/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /home/fatma/anaconda2/envs/conda_python_env/lib/python3.4/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1234vp66/model.ckpt-10000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-db6a26d3bf25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: {0:f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = learn.Estimator(model_fn=bag_of_words_model)\n",
    "# Train and predict\n",
    "classifier.fit(texts_train, target_train, steps=10000)\n",
    "y_predicted = [ p['class'] for p in classifier.predict(texts_test, as_iterable=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.822500\n",
      "auc: 0.647143\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.accuracy_score(target_test, y_predicted)\n",
    "auc = metrics.roc_auc_score (target_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))\n",
    "print('auc: {0:f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
